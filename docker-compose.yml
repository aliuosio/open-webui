services:

  open-webui:
    container_name: ${NAMESPACE}_open-webui
    image: ghcr.io/open-webui/open-webui:latest-cuda
    pull_policy: always
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia
    volumes:
      - ~/.webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: 1
              capabilities:
                - gpu

  ollama:
    container_name: ${NAMESPACE}_ollama
    image: ollama/ollama:latest
    pull_policy: always
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia
    tty: true
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: 1
              capabilities:
                - gpu

  fabric:
    build:
      context: ./.docker/fabric
      args:
        GO_IMAGE_VERSION: ${GO_IMAGE_VERSION}
    container_name: ${NAMESPACE}_fabric
    image: osioaliu/fabric:latest
    command: ["/bin/bash", "-c", "source /root/.bashrc && tail -f /dev/null"]
    # entrypoint: /root/entrypoint.sh
    depends_on:
      - ollama
    tty: true