services:

  webui:
    container_name: ${NAMESPACE}_webui
    image: ghcr.io/open-webui/open-webui:latest-cuda
    pull_policy: always
    tty: true
    env_file:
      - .env
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - ENABLE_RAG_WEB_SEARCH=True
      - RAG_WEB_SEARCH_ENGINE="searxng"
      - RAG_WEB_SEARCH_RESULT_COUNT=3
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
      - SEARXNG_QUERY_URL="http://searxng:8080/search?q=<query>"
    runtime: nvidia
    volumes:
      - ~/.webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: 1
              capabilities:
                - gpu

  ollama:
    container_name: ${NAMESPACE}_ollama
    image: ollama/ollama:latest
    pull_policy: always
    restart: unless-stopped
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia
    tty: true
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: 1
              capabilities:
                - gpu
                
  searxng:
    container_name: ${NAMESPACE}_searxng
    image: searxng/searxng:latest
    ports:
      - "8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    env_file:
      - .env
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
        
  fabric:
      container_name: ${NAMESPACE}_fabric
      image: osioaliu/fabric:latest
      pull_policy: always
      command: tail -f /dev/null
      depends_on:
        - ollama
      volumes:
        - ~/obsidian:/obsidian
      tty: true
      env_file:
        - .env

  python:
    build:
      context: .docker/python
      args:
        PY_VERSION: ${PY_VERSION}
    container_name: ${NAMESPACE}_python
    depends_on:
      - ollama
    image: osioaliu/python-rust:${PY_VERSION}
    tty: true
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - 'TZ=${TZ}'
      - 'OLLAMA_BASE_URL=${OLLAMA_BASE_URL}'
    working_dir: /usr/src/
    volumes:
      - ./src:/usr/src
    command: "tail -f /dev/null"