services:

  webui:
    container_name: ${NAMESPACE}_webui
    image: ghcr.io/open-webui/open-webui:latest-cuda
    pull_policy: always
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - ENABLE_RAG_WEB_SEARCH=True
      - RAG_WEB_SEARCH_ENGINE="searxng"
      - RAG_WEB_SEARCH_RESULT_COUNT=3
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
      - SEARXNG_QUERY_URL="http://searxng:8080/search?q=<query>"
    runtime: nvidia
    volumes:
      - ~/.webui:/app/backend/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: 1
              capabilities:
                - gpu

  ollama:
    container_name: ${NAMESPACE}_ollama
    image: ollama/ollama:latest
    pull_policy: always
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    runtime: nvidia
    tty: true
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: 1
              capabilities:
                - gpu
                
  searxng:
    container_name: ${NAMESPACE}_searxng
    image: searxng/searxng:latest
    ports:
      - "8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
        
  fabric:
      container_name: ${NAMESPACE}_fabric
      image: osioaliu/fabric:latest
      pull_policy: always
      command: ["/bin/bash", "-c", "tail -f /dev/null"]
      depends_on:
        - ollama
      volumes:
        - ./src:/src
        - ~/obsidian:/obsidian
      tty: true
  
 # docker run --rm -v ./src:/src/ -v cache:/app/cache ghcr.io/obeone/crawler-to-md --output-folder /src/crawler-output --url <URL>
# https://www.portainer.io/blog/stacks-docker-compose-the-portainer-way


  # python:
  #   build:
  #     context: .docker/python
  #     args:
  #       PY_VERSION: ${PY_VERSION}
  #   container_name: ${NAMESPACE}_python
  #   depends_on:
  #     - ollama
  #   image: osioaliu/python-rust:${PY_VERSION}
  #   tty: true
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   environment:
  #     - 'TZ=${TZ}'
  #     - 'OLLAMA_BASE_URL=${OLLAMA_BASE_URL}'
  #   working_dir: /src
  #   volumes:
  #     - ./src:/src
  #   #command: "tail -f /dev/null"

  # node:
  #   image: node:18
  #   container_name: ${NAMESPACE}_node
  #   working_dir: /usr/src/app
  #   volumes:
  #     - ./src:/src
  #   #ports:
  #   #  - "3000:3000"
  #   #command: ["npm", "start"]

  # portainer:
  #   container_name: ${NAMESPACE}_portainer
  #   image: portainer/portainer
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock
  #     - portainer_data:/data
  #   ports:
  #     - "9001:9000"
      
volumes:
  portainer_data: